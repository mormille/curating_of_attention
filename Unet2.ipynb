{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 180\n",
    "W=180\n",
    "\n",
    "transform = T.Compose([\n",
    "T.Resize((H,W)),\n",
    "T.ToTensor(),\n",
    "T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "training_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "validation_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(training_dataset, batch_size=10, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size = 10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datasets = {\n",
    "    'train': training_dataset, 'val': validation_dataset\n",
    "}\n",
    "\n",
    "dataloaders = {\n",
    "    'train': training_loader,\n",
    "    'val': validation_loader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convrelu(in_channels, out_channels, kernel, padding):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel, padding=padding),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetUNet(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base_model = models.resnet18(pretrained=True)\n",
    "        self.base_layers = list(self.base_model.children())\n",
    "\n",
    "        self.layer0 = nn.Sequential(*self.base_layers[:3]) # size=(N, 64, x.H/2, x.W/2)\n",
    "        self.layer0_1x1 = convrelu(64, 64, 1, 0)\n",
    "        self.layer1 = nn.Sequential(*self.base_layers[3:5]) # size=(N, 64, x.H/4, x.W/4)\n",
    "        self.layer1_1x1 = convrelu(64, 64, 1, 0)\n",
    "        self.layer2 = self.base_layers[5]  # size=(N, 128, x.H/8, x.W/8)\n",
    "        self.layer2_1x1 = convrelu(128, 128, 1, 0)\n",
    "        self.layer3 = self.base_layers[6]  # size=(N, 256, x.H/16, x.W/16)\n",
    "        self.layer3_1x1 = convrelu(256, 256, 1, 0)\n",
    "        self.layer4 = self.base_layers[7]  # size=(N, 512, x.H/32, x.W/32)\n",
    "        self.layer4_1x1 = convrelu(512, 512, 1, 0)\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        self.conv_up3 = convrelu(256 + 512, 512, 3, 1)\n",
    "        self.conv_up2 = convrelu(128 + 512, 256, 3, 1)\n",
    "        self.conv_up1 = convrelu(64 + 256, 256, 3, 1)\n",
    "        self.conv_up0 = convrelu(64 + 256, 128, 3, 1)\n",
    "\n",
    "        self.conv_original_size0 = convrelu(3, 64, 3, 1)\n",
    "        self.conv_original_size1 = convrelu(64, 64, 3, 1)\n",
    "        self.conv_original_size2 = convrelu(64 + 128, 64, 3, 1)\n",
    "\n",
    "        self.conv_last = nn.Conv2d(64, n_class, 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x_original = self.conv_original_size0(input)\n",
    "        x_original = self.conv_original_size1(x_original)\n",
    "\n",
    "        layer0 = self.layer0(input)\n",
    "        layer1 = self.layer1(layer0)\n",
    "        layer2 = self.layer2(layer1)\n",
    "        layer3 = self.layer3(layer2)\n",
    "        layer4 = self.layer4(layer3)\n",
    "\n",
    "        layer4 = self.layer4_1x1(layer4)\n",
    "        x = self.upsample(layer4)\n",
    "        layer3 = self.layer3_1x1(layer3)\n",
    "        x = torch.cat([x, layer3], dim=1)\n",
    "        x = self.conv_up3(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer2 = self.layer2_1x1(layer2)\n",
    "        x = torch.cat([x, layer2], dim=1)\n",
    "        x = self.conv_up2(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer1 = self.layer1_1x1(layer1)\n",
    "        x = torch.cat([x, layer1], dim=1)\n",
    "        x = self.conv_up1(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer0 = self.layer0_1x1(layer0)\n",
    "        x = torch.cat([x, layer0], dim=1)\n",
    "        x = self.conv_up0(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, x_original], dim=1)\n",
    "        x = self.conv_original_size2(x)\n",
    "\n",
    "        out = self.conv_last(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Installing collected packages: torchsummary\n",
      "Successfully installed torchsummary-1.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
      "              ReLU-2         [-1, 64, 224, 224]               0\n",
      "            Conv2d-3         [-1, 64, 224, 224]          36,928\n",
      "              ReLU-4         [-1, 64, 224, 224]               0\n",
      "            Conv2d-5         [-1, 64, 112, 112]           9,408\n",
      "            Conv2d-6         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-7         [-1, 64, 112, 112]             128\n",
      "       BatchNorm2d-8         [-1, 64, 112, 112]             128\n",
      "              ReLU-9         [-1, 64, 112, 112]               0\n",
      "             ReLU-10         [-1, 64, 112, 112]               0\n",
      "        MaxPool2d-11           [-1, 64, 56, 56]               0\n",
      "        MaxPool2d-12           [-1, 64, 56, 56]               0\n",
      "           Conv2d-13           [-1, 64, 56, 56]          36,864\n",
      "           Conv2d-14           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-15           [-1, 64, 56, 56]             128\n",
      "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
      "             ReLU-17           [-1, 64, 56, 56]               0\n",
      "             ReLU-18           [-1, 64, 56, 56]               0\n",
      "           Conv2d-19           [-1, 64, 56, 56]          36,864\n",
      "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
      "      BatchNorm2d-22           [-1, 64, 56, 56]             128\n",
      "             ReLU-23           [-1, 64, 56, 56]               0\n",
      "             ReLU-24           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-25           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-26           [-1, 64, 56, 56]               0\n",
      "           Conv2d-27           [-1, 64, 56, 56]          36,864\n",
      "           Conv2d-28           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-29           [-1, 64, 56, 56]             128\n",
      "      BatchNorm2d-30           [-1, 64, 56, 56]             128\n",
      "             ReLU-31           [-1, 64, 56, 56]               0\n",
      "             ReLU-32           [-1, 64, 56, 56]               0\n",
      "           Conv2d-33           [-1, 64, 56, 56]          36,864\n",
      "           Conv2d-34           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-35           [-1, 64, 56, 56]             128\n",
      "      BatchNorm2d-36           [-1, 64, 56, 56]             128\n",
      "             ReLU-37           [-1, 64, 56, 56]               0\n",
      "             ReLU-38           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-39           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-40           [-1, 64, 56, 56]               0\n",
      "           Conv2d-41          [-1, 128, 28, 28]          73,728\n",
      "           Conv2d-42          [-1, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-43          [-1, 128, 28, 28]             256\n",
      "      BatchNorm2d-44          [-1, 128, 28, 28]             256\n",
      "             ReLU-45          [-1, 128, 28, 28]               0\n",
      "             ReLU-46          [-1, 128, 28, 28]               0\n",
      "           Conv2d-47          [-1, 128, 28, 28]         147,456\n",
      "           Conv2d-48          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-49          [-1, 128, 28, 28]             256\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "           Conv2d-51          [-1, 128, 28, 28]           8,192\n",
      "           Conv2d-52          [-1, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "      BatchNorm2d-54          [-1, 128, 28, 28]             256\n",
      "             ReLU-55          [-1, 128, 28, 28]               0\n",
      "             ReLU-56          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-57          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-58          [-1, 128, 28, 28]               0\n",
      "           Conv2d-59          [-1, 128, 28, 28]         147,456\n",
      "           Conv2d-60          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-61          [-1, 128, 28, 28]             256\n",
      "      BatchNorm2d-62          [-1, 128, 28, 28]             256\n",
      "             ReLU-63          [-1, 128, 28, 28]               0\n",
      "             ReLU-64          [-1, 128, 28, 28]               0\n",
      "           Conv2d-65          [-1, 128, 28, 28]         147,456\n",
      "           Conv2d-66          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-67          [-1, 128, 28, 28]             256\n",
      "      BatchNorm2d-68          [-1, 128, 28, 28]             256\n",
      "             ReLU-69          [-1, 128, 28, 28]               0\n",
      "             ReLU-70          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-71          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-72          [-1, 128, 28, 28]               0\n",
      "           Conv2d-73          [-1, 256, 14, 14]         294,912\n",
      "           Conv2d-74          [-1, 256, 14, 14]         294,912\n",
      "      BatchNorm2d-75          [-1, 256, 14, 14]             512\n",
      "      BatchNorm2d-76          [-1, 256, 14, 14]             512\n",
      "             ReLU-77          [-1, 256, 14, 14]               0\n",
      "             ReLU-78          [-1, 256, 14, 14]               0\n",
      "           Conv2d-79          [-1, 256, 14, 14]         589,824\n",
      "           Conv2d-80          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-81          [-1, 256, 14, 14]             512\n",
      "      BatchNorm2d-82          [-1, 256, 14, 14]             512\n",
      "           Conv2d-83          [-1, 256, 14, 14]          32,768\n",
      "           Conv2d-84          [-1, 256, 14, 14]          32,768\n",
      "      BatchNorm2d-85          [-1, 256, 14, 14]             512\n",
      "      BatchNorm2d-86          [-1, 256, 14, 14]             512\n",
      "             ReLU-87          [-1, 256, 14, 14]               0\n",
      "             ReLU-88          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-89          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-90          [-1, 256, 14, 14]               0\n",
      "           Conv2d-91          [-1, 256, 14, 14]         589,824\n",
      "           Conv2d-92          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-93          [-1, 256, 14, 14]             512\n",
      "      BatchNorm2d-94          [-1, 256, 14, 14]             512\n",
      "             ReLU-95          [-1, 256, 14, 14]               0\n",
      "             ReLU-96          [-1, 256, 14, 14]               0\n",
      "           Conv2d-97          [-1, 256, 14, 14]         589,824\n",
      "           Conv2d-98          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-99          [-1, 256, 14, 14]             512\n",
      "     BatchNorm2d-100          [-1, 256, 14, 14]             512\n",
      "            ReLU-101          [-1, 256, 14, 14]               0\n",
      "            ReLU-102          [-1, 256, 14, 14]               0\n",
      "      BasicBlock-103          [-1, 256, 14, 14]               0\n",
      "      BasicBlock-104          [-1, 256, 14, 14]               0\n",
      "          Conv2d-105            [-1, 512, 7, 7]       1,179,648\n",
      "          Conv2d-106            [-1, 512, 7, 7]       1,179,648\n",
      "     BatchNorm2d-107            [-1, 512, 7, 7]           1,024\n",
      "     BatchNorm2d-108            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-109            [-1, 512, 7, 7]               0\n",
      "            ReLU-110            [-1, 512, 7, 7]               0\n",
      "          Conv2d-111            [-1, 512, 7, 7]       2,359,296\n",
      "          Conv2d-112            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-113            [-1, 512, 7, 7]           1,024\n",
      "     BatchNorm2d-114            [-1, 512, 7, 7]           1,024\n",
      "          Conv2d-115            [-1, 512, 7, 7]         131,072\n",
      "          Conv2d-116            [-1, 512, 7, 7]         131,072\n",
      "     BatchNorm2d-117            [-1, 512, 7, 7]           1,024\n",
      "     BatchNorm2d-118            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-119            [-1, 512, 7, 7]               0\n",
      "            ReLU-120            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-121            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-122            [-1, 512, 7, 7]               0\n",
      "          Conv2d-123            [-1, 512, 7, 7]       2,359,296\n",
      "          Conv2d-124            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-125            [-1, 512, 7, 7]           1,024\n",
      "     BatchNorm2d-126            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-127            [-1, 512, 7, 7]               0\n",
      "            ReLU-128            [-1, 512, 7, 7]               0\n",
      "          Conv2d-129            [-1, 512, 7, 7]       2,359,296\n",
      "          Conv2d-130            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-131            [-1, 512, 7, 7]           1,024\n",
      "     BatchNorm2d-132            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-133            [-1, 512, 7, 7]               0\n",
      "            ReLU-134            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-135            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-136            [-1, 512, 7, 7]               0\n",
      "          Conv2d-137            [-1, 512, 7, 7]         262,656\n",
      "            ReLU-138            [-1, 512, 7, 7]               0\n",
      "        Upsample-139          [-1, 512, 14, 14]               0\n",
      "          Conv2d-140          [-1, 256, 14, 14]          65,792\n",
      "            ReLU-141          [-1, 256, 14, 14]               0\n",
      "          Conv2d-142          [-1, 512, 14, 14]       3,539,456\n",
      "            ReLU-143          [-1, 512, 14, 14]               0\n",
      "        Upsample-144          [-1, 512, 28, 28]               0\n",
      "          Conv2d-145          [-1, 128, 28, 28]          16,512\n",
      "            ReLU-146          [-1, 128, 28, 28]               0\n",
      "          Conv2d-147          [-1, 256, 28, 28]       1,474,816\n",
      "            ReLU-148          [-1, 256, 28, 28]               0\n",
      "        Upsample-149          [-1, 256, 56, 56]               0\n",
      "          Conv2d-150           [-1, 64, 56, 56]           4,160\n",
      "            ReLU-151           [-1, 64, 56, 56]               0\n",
      "          Conv2d-152          [-1, 256, 56, 56]         737,536\n",
      "            ReLU-153          [-1, 256, 56, 56]               0\n",
      "        Upsample-154        [-1, 256, 112, 112]               0\n",
      "          Conv2d-155         [-1, 64, 112, 112]           4,160\n",
      "            ReLU-156         [-1, 64, 112, 112]               0\n",
      "          Conv2d-157        [-1, 128, 112, 112]         368,768\n",
      "            ReLU-158        [-1, 128, 112, 112]               0\n",
      "        Upsample-159        [-1, 128, 224, 224]               0\n",
      "          Conv2d-160         [-1, 64, 224, 224]         110,656\n",
      "            ReLU-161         [-1, 64, 224, 224]               0\n",
      "          Conv2d-162          [-1, 6, 224, 224]             390\n",
      "================================================================\n",
      "Total params: 28,976,646\n",
      "Trainable params: 28,976,646\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 417.65\n",
      "Params size (MB): 110.54\n",
      "Estimated Total Size (MB): 528.76\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ResNetUNet(n_class=3)\n",
    "model = model.to(device)\n",
    "\n",
    "# check keras-like model summary using torchsummary\n",
    "from torchsummary import summary\n",
    "summary(model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for param in model.parameters():\n",
    "#    print(param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "running_loss_history = [] # training loss - to generate a plot\n",
    "running_corrects_history = [] # traning accuracy\n",
    "val_running_loss_history = [] # validation loss\n",
    "val_running_corrects_history = [] # validation accuracy\n",
    "\n",
    "for e in range(epochs):\n",
    "  \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "    val_running_loss = 0.0\n",
    "    val_running_corrects = 0.0\n",
    "\n",
    "    for inputs, labels in training_loader:\n",
    "    #TRAINING\n",
    "        model.train()\n",
    "        inputs = inputs.to(device) # allow gpu use\n",
    "        labels = labels.to(device) # allow gpu use\n",
    "        outputs = unet(inputs) #gives the output of the last layer\n",
    "        loss = criterion(outputs, labels) # comparing outputs and labels using the criteria\n",
    "\n",
    "        optimizer.zero_grad() #zero the grad\n",
    "        loss.backward() #backpropagation\n",
    "        optimizer.step() #optimize weights \n",
    "\n",
    "        _, preds = torch.max(outputs, 1) # return the index of the maximum value predicted for that image (used to generate the accuracy)\n",
    "        running_loss += loss.item() # the sum of the loss of all itens\n",
    "        running_corrects += torch.sum(preds == labels.data) # the sum of correct prediction on an epochs\n",
    "\n",
    "    else:\n",
    "        #VALIDATION\n",
    "        model.eval()\n",
    "        with torch.no_grad(): # to save memory (temporalely set all the requires grad to be false)\n",
    "            for val_inputs, val_labels in validation_loader:\n",
    "                val_inputs = val_inputs.to(device) # allow gpu use\n",
    "                val_labels = val_labels.to(device) # allow gpu use\n",
    "                val_outputs = unet(val_inputs) #passes the image through the network and get the output\n",
    "                val_loss = criterion(val_outputs, val_labels) #compare output and labels to get the loss \n",
    "\n",
    "                _, val_preds = torch.max(val_outputs, 1) #same as for training\n",
    "                val_running_loss += val_loss.item() #same as for training\n",
    "                val_running_corrects += torch.sum(val_preds == val_labels.data) #same as for training\n",
    "\n",
    "    #TRAINING LOSS AND ACCURACY\n",
    "    epoch_loss = running_loss/len(training_dataset) # the sum of the loss of all itens divided by the number of itens\n",
    "    epoch_acc = running_corrects.float()/ len(training_dataset) # the sum of correct predictions divided by the number of itens\n",
    "    running_loss_history.append(epoch_loss) #append to respective list\n",
    "    running_corrects_history.append(epoch_acc) #append to respective list\n",
    "\n",
    "    #VALIDATION LOSS AND ACCURACY\n",
    "    val_epoch_loss = val_running_loss/len(validation_dataset)\n",
    "    val_epoch_acc = val_running_corrects.float()/ len(validation_dataset)\n",
    "    val_running_loss_history.append(val_epoch_loss) #append to respective list\n",
    "    val_running_corrects_history.append(val_epoch_acc) #append to respective list\n",
    "    print('epoch :', (e+1))\n",
    "    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
    "    print('validation loss: {:.4f}, validation acc {:.4f} '.format(val_epoch_loss, val_epoch_acc.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
